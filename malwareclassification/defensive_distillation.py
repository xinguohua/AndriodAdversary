from keras import Sequential
from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint
from keras.layers import Dense, Dropout
from keras.optimizers import Adam
import tensorflow as tf
from sklearn.model_selection import train_test_split
import os
import keras
import numpy as np

total_features = 25000  # total unique features
path = "defensive_distillation\\"

if not os.path.exists(path):  # check if path exists
    os.mkdir(path)


def train(train_data, train_labels, test_data, test_labels, file_name,
          epochs=4, batch_size=150, train_temp=1, init=None, callbacks=False):
    # neural net parameters
    units = [200, 200]
    activation_function = "relu"
    kernel = "glorot_uniform"
    bias = "zeros"
    dropout = 0.2
    learn_rate = 0.001

    model = Sequential()  # neural net init
    model.add(Dense(units=units[0], activation=activation_function, input_dim=total_features, kernel_initializer=kernel,
                    bias_initializer=bias))
    model.add(Dropout(dropout))  # add dropout rate

    for hidden_layer_units in units[1:]:  # add hidden layers defined units in train_models.py
        model.add(Dense(units=hidden_layer_units, activation=activation_function, kernel_initializer=kernel,
                        bias_initializer=bias))
        model.add(Dropout(dropout))

    model.add(Dense(2))  # output layer, with  with two neurons and without activation function

    if init is not None:
        model.load_weights(init)

    def fn(correct, predicted):
        return tf.nn.softmax_cross_entropy_with_logits(labels=correct, logits=(predicted / train_temp))

    # loss the fn method defined above, Adam optimizer
    model.compile(loss=fn,
                  optimizer=Adam(lr=learn_rate),
                  metrics=["accuracy"])

    if callbacks:
        log_dir = path + "log\\dir\\DNN"
        tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, write_images=True)
        early_stopping_callback = EarlyStopping(monitor='val_loss', mode='min', patience=10, verbose=2)
        model_checkpoint_callback = ModelCheckpoint(file_name, monitor='val_accuracy',
                                                    mode='max',
                                                    verbose=2, save_best_only=True)
        model.fit(train_data, train_labels,
                  epochs=epochs,
                  batch_size=batch_size,
                  validation_data=(test_data, test_labels),
                  callbacks=[tensorboard_callback, early_stopping_callback, model_checkpoint_callback],
                  verbose=2)
    else:
        model.fit(train_data, train_labels,
                  epochs=epochs,
                  batch_size=batch_size,
                  validation_data=(test_data, test_labels),
                  verbose=2)

        if file_name is not None:
            model.save(file_name)

    return model


def train_distillation(features, labels, file_name, epochs=4, batch_size=150, train_temp=1):
    """
    :param features: the train data
    :param labels: the train labels
    :param file_name: the file to save teacher and student
    :param epochs: number of epochs
    :param batch_size: batch size
    :param train_temp: temperature
    :return:
    """
    if not os.path.exists(file_name + "_init"):
        # train for one epoch to get a starting point
        train(features, labels, test_data, test_labels, file_name + "_init", 1, batch_size)

    # train the teacher at the given temperature
    print("Temperature:", train_temp)
    teacher = train(features, labels, test_data, test_labels, file_name + "_teacher", epochs, batch_size,
                    train_temp, init=file_name + "_init")

    predicted = teacher.predict(features)  # evaluate the labels at the given temperature
    print(predicted)

    with tf.compat.v1.Session() as sess:
        y = sess.run(tf.nn.softmax(predicted / train_temp))
        print(y)
        train_labels = y

    # train the student at temperature t
    student = train(features, train_labels, test_data, test_labels, file_name, epochs, batch_size,
                    train_temp, init=file_name + "_init")

    student.save('.//models//distillation_model.h5')
    # predict at temperature 1
    predicted = student.predict(features)
    print(predicted)



data = np.loadtxt(open("..//data//x_train01.csv", "rb"), delimiter=",", skiprows=0,dtype=np.float32)
labels = np.loadtxt(open("..//data//y_train01.csv", "rb"), delimiter=",", skiprows=0,dtype=np.int32)

#测试
# data = np.loadtxt(open("..//data//X_train.csv", "rb"), delimiter=",", skiprows=0,dtype=np.float32)
# labels = np.loadtxt(open("..//data//Y_train.csv", "rb"), delimiter=",", skiprows=0,dtype=np.int32)
#将训练数据拆分成0.8训练 0.2 验证
data, test_data, labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=0)  # 随机选择25%作为测试集，剩余作为训练集

# we use categorical_cross_entropy and thus, an encode to one hot labels is required 标签变成热编码的形式 恶意[0,1] 良性[1,0]
labels = keras.utils.to_categorical(labels, 2)
test_labels = keras.utils.to_categorical(test_labels, 2)


# first train with original temperature (= 1)
#训练得到原始模型 在温度等于1的情况下得到原始模型
train(data, labels, test_data, test_labels, path + "original", epochs=30, callbacks=True)
# train teacher and student networks with a predefined temperature
train_distillation(data, labels, path + "distilled-100", epochs=7, batch_size=150, train_temp=120)
