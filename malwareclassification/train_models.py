from malwareclassification import models
from malwareclassification import neural_network as NN
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np


def train_DNN_models():

    total_features = 25000

    # neural net parameters
    # units = [10,10, ]
    # units = [10, 200, ]
    # units = [200,10 ]
    # units = [50, 50]
    # units = [50, 200]
    # units = [200, 50]
    # units = [100, 200]
    # units = [200, 100]
    # units = [200,300]
    # units = [300, 200]
    units = [200, 200]
    # units = [200, 200，200]
    # units = [200,200,200,200]
    dropout = 0.2  # dropout rate to avoid over fitting (Note that dropout alone is not efficient)
    epochs = 10  # set maximum epochs to 20. If callbacks are specified Keras will automatically stop the procedure
    batch_size = 150  # we found that the batch size of 150 fits better in our task
    learn_rate = 0.001  # specify the learning rate according to the optimizer used
    kernel_initializer = 'glorot_uniform'  # weight initialization
    bias_initializer = 'zeros'  # bias initialization
    activation_function = 'relu'  # activation function



    # init the neural net
    model = NN.generate_neural_network(total_features, units, dropout, learn_rate, kernel_initializer,
                                     bias_initializer, activation_function)
    """
    train the neural network with the given model, epochs, batch size, train data-labels.
    Specify verbosity level, validation data, callbacks and plots (if needed).
    Default parameters:
    verbose=0, validation=False, val_data=None, val_labels=None, callbacks=False, plot_history=False
    example:
    NN.train_neural_network(model, epochs, batch_size, data, labels, verbose=0,
                            validation=True, val_data=test_data, val_labels=test_labels,
                            callbacks=True, plot_history=True)
    This is the main training stage and thus we want to save the best models at the 'right time'. This is done
    setting the callback to True. Keras will seek for the minimum validation loss and it saves the model with
    the highest validation accuracy.
    """
    NN.train_neural_network(model, epochs, batch_size, data, labels, verbose=2,
                            validation=True, val_data=test_data, val_labels=test_labels,
                            callbacks=True, plot_history=True)


# def train_GNB_models():
#
#     model = GNB.train_gaussian_naive_bayes_classifier(data, labels, save=True)  # train Naive Bayes
#     GNB.evaluate_gaussian_naive_bayes_classifier(model, test_data, test_labels)  # test performance
#
# def train_MNB_models():
#     model = MNB.train_multi_naive_bayes_classifier(data, labels, save=True)
#     MNB.evaluate_multi_naive_bayes_classifier(model, test_data, test_labels)
#
#
# def train_BNB_models():
#      model = BNB.train_bernoulli_naive_bayes_classifier(data, labels, save=True)
#      BNB.evaluate_bernoulli_naive_bayes_classifier(model, test_data, test_labels)
#
# def train_CNB_models():
#     model = CNB.train_complement_naive_bayes_classifier(data, labels, save=True)
#     CNB.evaluate_complement_naive_bayes_classifier(model, test_data, test_labels)
#
# def train_DT_models():
#      model = DT.train_decision_tree_classifier(data, labels, save=True)
#      DT.evaluate_decision_tree_classifier(model, test_data, test_labels)
#
# def train_RF_models():
#
#     model = RF.train_random_forest_classifier(data, labels, save=True)
#     RF.evaluate_random_forest_classifier(model, test_data, test_labels)
#
# def train_KNN_models():
#     model = KNN.train_knn_classifier(data, labels, save=True)
#     KNN.evaluate_knn_classifier(model, test_data, test_labels)
#
# def train_LR_models():
#      model = LR.train_logistic_regression_classifier(data, labels, save=True)
#      LR.evaluate_logistic_regression_classifier(model, test_data, test_labels)
#
def train_SVM_models():
    model = SVM.train_svm_classifier(data, labels, save=True)
    SVM.evaluate_svm_classifier(model, test_data, test_labels)
    val_data = pd.read_csv('..//data//x_test01.csv')
    val_labels = pd.read_csv('..//data//y_test01.csv')
    SVM.test_svm_classifier(model, val_data, val_labels)

if __name__ == "__main__":

    data = pd.read_csv('..//data//x_train01.csv')
    labels = pd.read_csv('..//data//y_train01.csv')
    #将训练数据拆分成0.8训练 0.2 验证
    data, test_data, labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=0)  # 随机选择25%作为测试集，剩余作为训练集



    # 训练DNN
    # train_DNN_models()

    # # initialize sklearn models (classic machine learning)
    # GNB = models.GaussianNaiveBayes()
    # MNB = models.MultinomialNaiveBayes()
    # CNB = models.ComplementNaiveBayes()
    # BNB = models.BernoulliNaiveBayes()
    # DT = models.DecisionTree()
    # RF = models.RandomForest()
    # KNN = models.KNearestNeighbors()
    # LR = models.LogRegression()
    SVM = models.SupportVectorMachine()
    #
    # #     for classic machine learning model, e.g., Naive Bayes, Decision Tree etc, first we fit the classifier and
    # #     then we evaluate on the test. The best hyperparameters found from the grid search procedure are defined
    # #     in the models.py helper script.
    # #训练机器学习模型
    # train_GNB_models()
    # train_MNB_models()
    # train_CNB_models()
    # train_BNB_models()
    # train_DT_models()
    # train_RF_models()
    # train_KNN_models()
    # train_LR_models()
    train_SVM_models()

