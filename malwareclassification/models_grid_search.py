"""
This file performs grid search for 'classic' machine learning algorithms.
"""
##用于搜索机器学习算法的超参数
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV
from sklearn import svm
from sklearn.tree import DecisionTreeClassifier
import numpy as np

def grid_RF():
    print("--- Random Forest ---")
    n_estimators = [10, 50, 100, 200]  # number of trees
    criterion = ['gini', 'entropy']  # measurement for the quality of split
    max_features = ['sqrt', 'log2', None]  # Number of features to consider at every split
    min_samples_split = [2, 5, 10]  # Minimum number of samples required to split a node
    min_samples_leaf = [1, 2, 4]  # Minimum number of samples required at each leaf node
    # Create the grid
    param_grid = dict(n_estimators=n_estimators, criterion=criterion, max_features=max_features,
                      min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)

    rf = RandomForestClassifier()  # create the base model to tune
    # Use the grid search to search for best hyperparameters, using 3-fold cross validation
    rf_random = GridSearchCV(estimator=rf, param_grid=param_grid, cv=4, verbose=1,
                             n_jobs=1)  # Fit the grid search model
    grid_result = rf_random.fit(data, labels)
    print("Best: ", grid_result.best_score_, "using", grid_result.best_params_)  # find the best hyperparameter


def grid_KNN():
    print("--- K Nearest Neighbors ---")
    n_neighbors = [3, 5, 10, 20, 50]  # number of neighbors
    weights = ['uniform', 'distance']  # weight function to use in prediction
    metric = ['euclidean', 'manhattan', 'minkowski']  # distance metric to use

    param_grid = dict(n_neighbors=n_neighbors, weights=weights, metric=metric)

    knn = KNeighborsClassifier()

    knn_grid = GridSearchCV(estimator=knn, param_grid=param_grid, cv=4, n_jobs=-1)
    grid_result = knn_grid.fit(data, labels)
    print("Best: ", grid_result.best_score_, "using", grid_result.best_params_)


def grid_LR():
    print("--- Logistic Regression ---")
    C = [0.5, 1.0, 1.5, 2.0, 2.5]  # regularization strength
    max_iter = [100, 110, 120, 130, 140]  # maximum number of iterations
    fit_intercept = [True, False]  # add a bias or not to the decision function

    param_grid = dict(max_iter=max_iter, C=C, fit_intercept=fit_intercept)

    lr = LogisticRegression(penalty="l2", solver="lbfgs")

    grid = GridSearchCV(estimator=lr, param_grid=param_grid, cv=4, n_jobs=-1, verbose=1)
    grid_result = grid.fit(data, labels)
    print("Best: ", grid_result.best_score_, "using", grid_result.best_params_)


def grid_SVM():
    print("--- Support Vector Machines ---")
    C = [0.25, 0.5, 1.0]  # penalty parameter
    kernel = ['linear', 'rbf', 'poly']  # kernel type
    gamma = ['auto', 'scale']  # kernel coefficient
    decision_function_shape = ['ovo', 'ovr']  # one vs rest or one vs one

    param_grid = dict(C=C, kernel=kernel, gamma=gamma, decision_function_shape=decision_function_shape)

    SVM = svm.SVC()

    grid = GridSearchCV(estimator=SVM, param_grid=param_grid, cv=4, n_jobs=1, verbose=1)
    grid_result = grid.fit(data, labels)
    print("Best: ", grid_result.best_score_, "using", grid_result.best_params_)


def grid_DT():
    print("--- Decision Tree ---")
    criterion = ['gini', 'entropy']  # measurement for the quality of split
    splitter = ['best', 'random']
    max_features = ['sqrt', 'log2', None]  # Number of features to consider at every split
    min_samples_split = [2, 5, 10]  # Minimum number of samples required to split a node
    min_samples_leaf = [1, 2, 4]  # Minimum number of samples required at each leaf node

    param_grid = dict(criterion=criterion, splitter=splitter, max_features=max_features,
                      min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)

    DT = DecisionTreeClassifier()

    rf_random = GridSearchCV(estimator=DT, param_grid=param_grid, cv=4, n_jobs=1)
    grid_result = rf_random.fit(data, labels)
    print("Best: ", grid_result.best_score_, "using", grid_result.best_params_)


if __name__ == "__main__":
    total_features = 25000  # total unique features
    data = np.loadtxt(open("..//data//x_train01.csv", "rb"), delimiter=",", skiprows=0)
    labels = np.loadtxt(open("..//data//y_train01.csv", "rb"), delimiter=",", skiprows=0)
    print("Grid searching...")

    grid_RF()
    #grid_KNN()
    #grid_LR()
    #grid_SVM()
    #grid_DT()
