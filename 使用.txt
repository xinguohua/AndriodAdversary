2）网格搜索
在根据ML算法提供数据之前，我们必须为每个模型确定最佳超参数。此操作的目标是构建性能尽可能高的模型。

注意，此过程的结果不会产生每个模型的最终性能；
相反，参数调整仅用于观察每个模型的超参数中的最佳组合。


2.1）神经网络：
不幸的是，当尝试使用Scikit-learn封装了Keras方法的GridSearchCV模块进行自动网格搜索时，该方法在具有2000个样本的集中进行了3倍交叉验证，该模型无法容纳在我们的内存中。

通过这种方式，我们可以手动实现参数调整。在每个试验中，我们使用相同的样本作为训练集，共使用2000个应用程序，恶意软件比率为0.3，并使用3倍交叉验证。有关超参数值的更多详细信息，我们建议阅读Keras官方文档。

在我们的虚拟网络体系结构中，我们使用一个输入层，该层获取要素的总数（545,333），并将其馈送到2个隐藏层，其中每层包含200个神经元，而在输出层则输出2个神经元，以处理二进制分类任务。


至于确定学习过程的值，我们使用ADAM优化器在隐藏层中使用ReLU激活函数，在输出层中使用Softmax激活函数，dropout为0.5。

最后，我们定义稀疏分类交叉熵来计算损失
因为我们的目标类别是互斥的整数（每个样本都属于一个类别，并且是良性（0类）或恶意（1类）），而准确性则作为评估指标。

在每次试验中，我们都会更改参数值，以确定最适合我们任务的参数。
请注意，学习率以及权重和偏差的初始化值必须进行调整。
请注意，没有用于选择超参数组合的硬定义规则。
每个数据集的最佳组合都不同，并且参数需要对其进行调整。

我们发现最佳架构是2个隐藏层，每层包含200个神经元，而使用Adam优化器的最佳超参数为：


• batch size: 150

• epochs: 5

• dropout rate: 0.2

• activation function in hidden layers: ReLU

• weights initialization: glorot_uniform

• bias initialization: zeros

• learning rate: 0.001


最后，我们使用了相同的体系结构，批处理大小， epochs, dropout ，激活函数和权重，并对初始化进行了偏倚，以与其他优化程序一起训练具有相同体系结构的神经网络。

结果表明，即使超参数适应了大多数优化程序，其性能也相当不错，并且在精度方面达到了Adam。如果每个网格搜索过程都单独重复进行，则其余的优化器的性能会更好。



2.2) Classic ml
网格搜索过程也适用于所考虑的其他模型，即决策树，随机森林，k最近邻，逻辑回归和支持向量机。 请注意，贝叶斯分类器只有一个用于平滑数据的超参数，我们将此参数保留为其默认值。

python3 models_grid_search.py

决策树：
•标准：基尼

•分离器：最佳

•max_features：无

随机森林
•n_estimators：10

•标准：熵

•max_features：log2

k最近邻居
•n_neighbors：3

•重量：远近


•指标：minkowsi

逻辑回归
•C：2.0

支持向量机
•内核：线性

•C：0.5




3) Training models
在主要的训练过程中，我们使用1500个应用程序训练模型，并使用1500个应用程序的另一个样本对其进行评估。

 正如在寻找最佳超参数的过程中一样，我们在训练神经网络上投入了更多的精力。

 Keras支持早期停止技术。

我们将验证损失指定为要最小化的性能指标。 Keras力求将验证损失降到最低，并根据“耐心”论证在没有进一步改进的情况下自动停止训练过程。

耐心性参数指示Keras无法确定任何改善的时期的延迟。我们将纪元数定义为20，将耐心值定义为10。

 这意味着我们的模型最多可以训练20个纪元，但是如果在耐心值达到之前没有发现性能改善，该过程将自动停止。


最小的验证损失不能保证模型在验证集的准确性方面具有最佳性能。


 因此，我们定义了ModelCheckpoint回调，该回调控制最大的验证性能，并且每当输出增加时，就会存储模型。


python3 train_models.py


我们为Keras支持的每个优化程序实现了上述技术。 结果如下：

Optimizer	Min. Val. loss	Epoch	Checkpoint Acc.	Epoch	Val. loss
RMSprop	0.1792	2	94.67	5	0.2006
Adagrad	0.1746	3	94.67	4	0.1904
Adadelta	0.1654	6	94.66	15	0.2158
Adamax	0.1712	4	94.87	8	0.2038
NAdam	0.1853	3	94.73	6	0.2346
Adam	0.1867	4	94.47	4	0.1867
SGD	0.1632	15	94.2	20	0.1653
SGD with momentum	0.1653	9	94.33	15	0.1730



至于其他分类器，我们不会使用特定方法检查它们是否过拟合。
我们实施随机二次抽样技术，并根据平均准确度和标准差评估模型。

我们还使用在上一步中获得最高准确度的历元值来评估我们的每个神经网络模型。

对随机集的训练过程重复8次，随机选择一个值。

python3 random_subsampling.py

Model	Avg. Acc.	St. Dev
NB	92.32	0.91
DT	92.84	0.55
RF	93.48	0.72
k-NN	92.95	0.62
LR	94.64	0.55
SVM	93.9	0.35
RMSprop	94.28	0.69
Adagrad	94.79	0.67
Adadelta	94.2	0.45
Adamax	94.87	0.39
NAdam	94.9	0.55
Adam	94.53	0.84
SGD	94.65	0.5
SGD with momentum	94.5	0.7




我们观察到在随机样本中进行训练不会产生任何明显的性能差异。 因此，我们从数据集中随机选择了1500个样本作为训练集，其他1500个样本作为测试集。 请注意，当实施早期停止和ModelCheckpoint回调时，在前一过程中训练并存储的神经网络模型具有完全相同的训练和测试集。

python3 train_models.py


Model	Accuracy	FPR	FNR
NB	92.7	2.8	18
DT	90.8	7.8	12.4
RF	93.33	3	15
k-NN	93.67	2.95	14
LR	94.13	4.5	8.8
SVM	93.8	5.5	7.7
RMSprop	94.67	4.1	8.2
Adagrad	94.66	4.7	6.6
Adadelta	94.67	4.1	8
Adamax	94.87	3.7	8.9
NAdam	94.73	4.3	7.3
Adam	94.47	3.6	10
SGD	94.2	4.3	9
SGD with momentum	94.33	3	9.7



尽管已经显示出模型在不同的集合中具有相似的行为，但是有必要证明训练后的模型在除1500个应用程序的集合之外的其他随机集合中表现良好。 这是为了确认模型确实能够将恶意软件与良性应用程序区分开来。 我们从1500个随机应用程序中随机选择8组，并评估每种训练模型的性能。


Model	Avg. Acc.	Avg. FPR	Avg. FNR
NB	92.09	2	21.5
DT	91.73	6	13.5
RF	93.72	2.76	14.4
k-NN	93.67	3	14
LR	94.6	3.8	8
SVM	93.9	5.2	7.5
RMSprop	94.82	3.8	8.3
Adagrad	94.88	4.4	6.6
Adadelta	94.84	3.4	9.2
Adamax	95.11	3.5	8.1
NAdam	94.6	4.5	7.5
Adam	94.53	3.1	10.8
SGD	94.36	3.9	9.58
SGD with momentum	95.05	3.2	8.7


4) Incremental Learning
这些算法可以一次动态适应新的样本和模式，而不是一次1在整个数据集上或在一个数据集的样本上学习（如先前的评估所示）。

朴素贝叶斯和神经网络就是这种情况。


对于决策树，随机森林，k最近邻，逻辑回归和基于支持向量机的分类器，有任何最新的增量学习实现。


数据被馈送到算法的方式使得所有恶意软件应用程序仅一次通过学习算法。

我们通过每次迭代提供1,000个样本来实现这一目标，其中300个样本是恶意的。

迭代总数为19，恶意软件样本总数为5,560，良性样本总数为13,440。

 但是，我们不能依赖此技术的结果，因为野外的恶意软件总量非常庞大，无法仅用5,560个恶意应用程序替代。

增量学习程序主要用于观察对手在场时模型的行为。


至少在神经网络中，该性能大大提高，在以前的评估中使用的1500个应用程序的同一验证集中，该性能达到了约98.5％。


python3 incremental_learning.py
Model	Accuracy	FPR	FNR
NB	93.13	6.8	7.1
RMSprop	97.67	1.1	5.1
Adagrad	97.93	1.1	4.2
Adadelta	98.13	0.95	4
Adamax	98.13	1.1	3.6
NAdam	98.4	1	2.8
Adam	98.46	1.3	2
SGD	97.4	1.4	5.3
SGD with momentum	97.3	1.5	5.3





5）使用FGSM制作对抗示例

该攻击基于Tensorflow教程。

首先，我们得到损失相对于输入的梯度。


其次，我们通过将损失的梯度与噪声ε相乘并将结果添加到原始输入中来产生对抗性示例。

计算机视觉领域中的epsilon相对较小，因为目标是产生可以欺骗机器学习模型而不会影响人类决策的输入，因为它们看起来不会改变图像的外观。


在恶意软件分类中，epsilon可以是任意大小，因为人们甚至无法理解原始数据本身。

但是，我们坚持原始方法，并且将epsilon的值保持在较低的水平，特别是0.01。

我们评估了由1000个随机应用程序的测试集生成的神经网络模型，恶意软件比率为0.3。

我们还使用相同的测试集评估增量学习模型。

python3 fgsm.py

所有训练有素的样本为1500的模型都将失去作用，因为它们只能达到5.5％的精度（使用NAdam优化器）。

SGD优化器仅以0.1％的精度执行最差的性能。

对于受欺骗程度最高的模型，使用Adadelta优化器训练的模型受到的影响最大，错误分类率为94.4％。

 错误分类率的计算是基于模型的原始准确性及其在对抗性示例中的准确性。

这不能使我们得出结论，在对抗性示例中优化器是否会影响模型的性能，因为它们的错误分类率均大于90％。



Model	Accuracy	FPR	FNR	Adv. Accuracy	Adv. FPR	Adv. FNR	MR
RMSprop	95.3	3.71	7	2.7	96.14	100	92.6
Adagrad	95.4	4.1	5.6	2.5	96.86	99	92.9
Adadelta	95.2	3.6	7.7	0.8	98.8	100	94.4
Adamax	95.4	3.3	7	3.5	95	100	91.9
NAdam	95.6	3.9	5.6	5.5	93.72	96.33	90.1
Adam	95.1	3.3	8.7	2.5	96.42	100	92.6
SGD	94.3	4.6	8.3	0.1	99.9	100	94.2
SGD with momentum	94.6	3.6	9.6	0.3	99.57	100	94.3



至于增量学习模型，其结果非常相似，只是在准确性方面稍有提高。

使用RMSprop优化器训练的模型在对抗性方面表现最佳，达到28.8％的精度，而Adagrad表现最差，达到0.89％的精度。


Model	Accuracy	FPR	FNR	Adv. Accuracy	Adv. FPR	Adv. FNR	MR
RMSprop	98.6	0.42	3.7	2.7	96.14	100	95.9
Adagrad	98.7	0.57	3	0.89	98.71	100	97.81
Adadelta	98.5	0.42	3.6	21.4	76.71	83	77.4
Adamax	98.6	0.85	7	8.9	8.9	90	89.7
NAdam	98.8	0.85	5.6	13.5	80.43	94	85.3
Adam	98.8	0.85	8.7	8	8	91.86	90.8
SGD	98.3	4.6	0.85	6.3	6.3	98	92
SGD with momentum	98.3	4	9.6	7.1	91.14	97	91.2


FGSM攻击完全基于损失函数的梯度，这是恶意软件分类任务的不现实场景，因为它会向输入域随机添加噪声。

使用这种方法，最有可能的是将要创建的应用程序无法运行（尽管能够欺骗机器学习模型）。

这不是对手想要的，因为目的不仅在于欺骗，而且还在于维护应用程序的功能。

但是，已知模型参数的对手可以创建恶意应用程序的副本，并且只要应用程序经过检测器，对手就可以推送由FGSM攻击创建的功能。

因此，它可以超越检测器。

这是一个困难的成就，因为对手必须以某种方式推动这些功能。

因此，基于原始恶意软件创建具有绕过检测器属性的新应用程序是一种更实际的方案。

6）使用JSMA制作对抗示例、

该攻击基于JSMA变体。

这些步骤与JSMA的原始提议相似，不同之处在于我们处理二进制分类任务。

这样，在第一步中，攻击基于预测的类别F（x），输入维度m（= 545,333个特征）和2个类别的输出维度，使用了模型F的前向导数。

在每个输入数据中都进行正向导数的计算，分别评估每个输入中模型的结果。

本质上，前导数通过首先计算模型相对于输入的梯度来估计扰动将改变模型结果的方向。

第二步是找到最大正斜率的x中的小变化δ到目标类别y'= 0。

换句话说，我们通过改变索引i处x的值来计算索引i，以最大程度地改变目标类。

此方法的局限性是仅当最大索引值为0时才发生更改。

这意味着我们仅添加功能而不删除任何功能，将x_i的值从0更改为1。

使用这种方法，我们可以确保应用程序至少在我们的恶意软件检测器中保持功能正常并能够绕过安全机制。

 本质上，我们为样本x获得了一个新的特征向量。这仅适用于输入维的一个更改，但如此小的更改（仅更改一个值）不能确保可能导致模型对样本进行错误分类。因此，我们需要进行更多更改，直到实现错误分类。

 为此，我们重新计算了新输入向量下的梯度，并发现了另一个要更改的特征。重复该过程，直到达到错误分类或更改数量达到我们允许的最大更改数量限制为止。最大变化的值是此方法中的第二个限制。为了确保算法在原始特征向量中不会产生超过20个扰动，我们将最大变化设置为k = 20。


python3 jsma.py



首先，我们评估以1500个应用样本为基础训练的模型。 正如所观察到的那样，使用JSMA变体时，最可靠的模型是使用Adagrad优化器的模型，该模型可实现71.2％的总体性能和86.3％的假阴性率，平均变化为12.3。


Model	Accuracy	FNR	Adv. Accuracy	Adv. FNR	MR	Distortion
RMSprop	95.3	7	68.5	96.3	89.3	7.34
Adagrad	95.4	5.6	71.2	86.3	80.7	12.3
Adadelta	95.2	7.6	68.5	97	89.4	7.54
Adamax	95.4	7	68.5	97.3	90.3	8.92
NAdam	95.6	5.6	68.6	96	90.4	8.89
Adam	95.1	8.7	71.3	88	79.3	11.33
SGD	94.3	8.3	68.3	95	86.7	7.8
SGD with momentum	94.6	9.6	68.5	97	87.4	6.7

其次，我们评估增量学习模型。 最强大的模型是带有动量优化器的SGD，它仅以6.9的平均变化即可达到71％的整体性能和95％的假阴性率。
Model	Accuracy	FNR	Adv. Accuracy	Adv. FNR	MR	Distortion
RMSprop	98.6	3.7	69.7	100	96.3	5.71
Adagrad	98.7	3	70.6	97	94	7.24
Adadelta	98.5	3.7	70.4	97.3	93.6	5.8
Adamax	98.6	3.7	70.1	98.6	94.9	6.61
NAdam	98.8	2	69.4	100	98	5.95
Adam	98.8	2	69.6	99.3	97.3	7.21
SGD	98.3	3.7	70.6	96	92.3	6.3
SGD with momentum	98.3	4	71	95	91	6.9



7) 通过对抗训练进行防御。

建立模型的步骤：

1 训练分类器F。
2 使用JSMA制作方法制作F的对抗示例A。
3 将上一步中产生的A与F一起使用其他训练时期，作为与恶意类别相对应的训练数据。

我们在增量学习模型中采用对抗训练的方法经过Adam 优化器培训整个功能空间。
请注意，通过ModelCheckpoint回调，我们发现使用Adam优化器（至少在整个特征空间的训练过程中），每次迭代4个epochs 可以产生具有最高准确性的模型。
因此，我们迭代了4个额外的epochs 来训练带有对抗性示例的模型。

对抗性训练可以遵循任何特定的方法，例如需要多少干扰，或者是否将对抗性示例与合法输入数据混合。



我们分别以1.0、0.7、0.5和0.3的比率与对抗性示例迭代4个额外的时期。
其余训练集充满了良性应用。
在每个试验中，对抗性示例都是从一组随机的恶意应用程序中产生的。

python3 adversarial_train.py


通过以0.3的对抗性示例比率（准确度：98.4，FPR：1.42，FNR：2）对800个样本（240个对抗性样本和560个合法的良性样本）进行额外训练，可以在原始示例的对抗性示例上实现最佳的总体性能。

但是，要确定哪个模型表现最好，仅针对其他模型制作的对抗示例中的评估是不够的。
因此，我们在没有对手的情况下将生成的模型的性能评估为随机集。

确定在总共有800个样本的对抗性示例中的0.3比率在有或没有对手的情况下均产生了性能最佳的模型，因为它在8个随机验证集中达到了98.52％的平均准确度。

至于用于评估原始模型的1500个验证集，结果非常相似。

在对抗性示例中训练比例为0.3的模型中，我们可以观察到平均性能略有下降（从98.46％降至97.73％），而FPR则有所提高（从1.3％降至2.48％），而FNR则略有下降（从2％到1.78％）。

 因此，通过额外训练800个样本（其中240个是对抗性样本），我们得到的模型具有与原始模型一样高的性能。

强大到足以欺骗原始模型的对手可能会欺骗对手训练的模型。

在以前的评估中，我们没有考虑训练有素的网络对专门为这些模型设计的对抗性示例的鲁棒性，而只是考虑了它们对原始模型的对抗性示例的抵抗力。

因此，我们为带有JSMA变体的其他经过训练的模型制作了对抗示例，以评估其对抗示例的鲁棒性。

结果表明，随着平均扰动量的增加，对抗训练的模型比原始模型更难以攻击，但误分类率仍然很高。

最健壮的模型似乎是训练有800个样本的模型，其对手例比率为0.5，因为在有对手的情况下它达到约61％的假阴性比率，需要平均15.24的变化。

易于观察到的是，针对原始模型制作的对抗示例进行训练并不能提高新模型对为其制作的对抗示例的抵抗力，尽管它们使对手更难找到扰动。


对抗训练的目标是产生一个模型，该模型不仅可以更好地推广用于原始模型的对抗示例，而且可以更好地推广对抗训练的模型。


因此，我们将对抗训练程序连续应用于新生成的网络。

我们将此方法应用于经过0.3个对抗示例比率训练的网络，因为它可以在没有对手的情况下实现更高的性能集。

我们的目标是在合法样品中生成与原始模型一样高的精度的模型，同时在对付对抗性示例的分类中实现较高的性能，并要求尽可能多的干扰。

为了实现这一点，我们为新生成的分类器F'计算新的对抗性示例。样品被反馈以重新训练F'并生成一个新的网络。我们不断应用此方法，直到出现高失真。经过12次迭代后，我们观察到了最健壮的模型，该模型需要欺骗17.21个平均变化。通过反复的对抗训练，我们观察到该模型不仅针对对抗示例变得更加健壮，而且在没有对手的情况下也具有与原始模型相似的行为。 FPR受到轻微影响（从0.85％降低到1.42％），但是我们可以观察到FNR降低（从2％降低到1％）。因此，对抗性训练使该模型在检测恶意应用程序方面更加强大。但是，尽管需要更多的干扰，但对手仍然能够绕过检测器。错误分类率从97.3％降至29.33，但仍然很高。




8) Defense with Distillation

建立模型的步骤：

1 使用标准方法训练分类器F（教师网络），除了最终的分类不是由通常的softmax激活函数给出，而是由softmax加上温度（T）变量给出的。通常，T是一个大值，并且当T→∞时，分布趋于均匀。我们在实验中评估了不同的T值。

2 在训练集的每个样本上评估上一步的训练分类器F。
在训练集的每个样本上评估上一步的训练分类器F。

结果，它创建了两个新标签，这些标签与样本属于良性或恶意类的可能性有关（软标签）。 换句话说，软标签不是生成反映模型对样本属于一个类别的信念的输出，而是包含样本属于一个或另一个类别的可能性。 例如，应用程序有70％的更改为恶意类，有30％的可能性为良性类。

没有T值，分类器应预测该样本是恶意应用程序。 取而代之的是，对于较大的T，分类器将输出与类相对应的概率。

3使用由分类器F产生的软标签上的温度T训练第二个分类器F'（学生/蒸馏网络）。

4 对每个样本的T = 1评估分类器F'。


我们只考虑使用训练样本为1500的Adam优化器实施防御性蒸馏。由于计算成本高，我们不采用增量学习来实现防御机制。
回想一下，在主要训练过程中（没有对手的情况下），我们使用稀疏分类交叉熵作为度量损失的函数，因为我们必须处理互斥整数（良性类为0，恶意类为1） 。


防御性蒸馏要求模型相信样本属于一个类别。

为此，我们使用基于对数的分类交叉熵来衡量损失。 logits表示该函数基于最后一个隐藏层的输出进行操作。 换句话说，使用logits时，最终输出值可能不等于1。

此外，分类交叉熵的使用要求标签为一热编码形式。 损失函数的这种变化预计会稍微影响模型的准确性，但不会有很大的差异。

因此，我们利用EarlyStopping和ModelCheckpoint回调来确定生成最准确模型的时期的值。


python3 defensive_distillation.py


我们发现在这种情况下，最佳epoch 为7。实际上，性能并未受到显着影响，因为在1500个应用程序的验证集上，该模型的性能比原始模型（原始94.47的94.6）略高。

在防御性蒸馏中，温度参数是影响对抗性示例中模型最终弹性的参数。因此，我们在不同温度下进行实验，将其值从10更改为200。

在防御性蒸馏中，温度参数是影响对抗性示例中模型最终弹性的参数。因此，我们使用不同的温度进行实验，将其值从10变为200。更具体地说，我们针对以下温度测量错误分类率：{10、20、40、70、100、120、150、200}。随着温度值的升高，防御性蒸馏可以抵御对抗性例子。但是，错误分类率仍然很高。发现最低的误分类率是在温度150，即8％。

有对抗性示例的情况下，整体表现为91.5％（FNR为20％）。恶意软件分类域中的FNR很高，因为目标是有效检测恶意应用程序。

总体而言，在防御性蒸馏中，随着温度参数的增加，在没有对手的情况下，FNR会增加。另一方面，FPR不受影响，因为我们观察到随着温度的升高略有降低。温度值为150时，错误分类率显着下降至8％。但是，能够产生错误分类的特征空间中的平均变化很小（温度为150时为2.29）。

9) Ensemble Classifier
通过合并多个分类器，我们希望提高整体性能，并减少对抗性示例中模型的错误分类率。
我们以这样一种方式组合分类器，即每个分类器对最终预测的贡献均等。结果，最终结果是多数表决方法。
对于最终预测，集合中的每个模型分别评估样本并输出其预测

我们以200个应用程序的小型测试集评估整体方法，其中100个是恶意软件。我们同时使用深度模型和经典机器学习算法来形成整体。

为了制作整体的对抗示例，我们使用基于由Adam优化器训练的深度模型的JSMA变体。

换句话说，我们假设一个对手不知道仅针对使用基于Adam优化器的深度模型的Adam优化器替代模型训练的模型的整体对抗模型。

在对抗性示例中，该模型的原始准确性为98％（FNR为2％）和49％（FNR为100％）。

python3 ensemble.py

所有测试配置的性能都非常相似，因为它们可以实现高于97％的精度和高达98.8％6的精度。 实际上，总体性能比仅使用模型的性能稍好。 此外，整体学习的确提高了对抗性例子的抵抗力。 仅使用深度学习模型，最高错误分类率为25％。 将计算损失和梯度的深度模型（例如Adam和SGD）与经典的机器学习算法相结合，可以生成足够健壮的模型。 通过将两个深度学习模型与logistic回归算法相结合，可以找到整体最优集合，在对抗性示例中，原始精度达到98.5％，错误分类率达到8％。 但是，必须执行调整策略来确定最佳的集成方法。